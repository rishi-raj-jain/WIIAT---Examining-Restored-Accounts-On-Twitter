{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt \n",
    "# %matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import os\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# import seaborn as sn\n",
    "import csv\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import *\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from pprint import pprint\n",
    "import tweepy\n",
    "import random\n",
    "import emoji\n",
    "import json\n",
    "import re\n",
    "import subprocess\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from dateutil.parser import parse\n",
    "import requests\n",
    "import plotly.graph_objects as go\n",
    "import csv\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid.inset_locator import inset_axes\n",
    "\n",
    "def plot_creation_date(dt_col,label,x1=1,x2=1,y1=1,y2=1):\n",
    "    ser = pd.Series(1, index=dt_col).resample('M', how='count')\n",
    "    ser.index = ser.index.to_period('M')\n",
    "    tot_counts = ser.sum()\n",
    "    ser /= tot_counts\n",
    "#     print(ser)\n",
    "    ax = ser.plot()\n",
    "    ax.set(xlabel=\"Creation_Date\", ylabel=\"Percentage of Accouts Created\")\n",
    "    ax.set_title(label)\n",
    "    axins = inset_axes(ax, 2,1.5 , loc=2,bbox_to_anchor=(0.2, 0.85),bbox_transform=ax.figure.transFigure) # no zoom\n",
    "#     axins = zoomed_inset_axes(ax, 2.0, loc=2) # zoom-factor: 2.5, location: upper-left\n",
    "    axins = ser.plot(marker='o')\n",
    "#    \n",
    "    axins.set(xlabel=\"\")\n",
    "    \n",
    "#    x1, x2, y1, y2 = 2018, 60, 3.7, 4.6 # specify the limits\n",
    "    axins.set_xlim(x1, x2) # apply the x-limits\n",
    "#     axins.set_ylim(y1, y2) # apply the y-limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pd(fpath):\n",
    "    df = pd.read_csv(fpath)\n",
    "    df_old = df[df['Filename'].str.contains(\"old\")] \n",
    "    df_new = df[df['Filename'].str.contains(\"new\")]\n",
    "\n",
    "    df_new['Filename'] = df_new.apply(lambda row : row['Filename'].split(\"_\")[0], axis = 1 ) \n",
    "    df_old['Filename'] = df_old.apply(lambda row : row['Filename'].split(\"_\")[0], axis = 1 ) \n",
    "    return df_new , df_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new,df_old = read_pd(\"../personality_analysis/old_work/results/liwc_sus.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired Dependent T-value\n",
    "#### Examples for use are scores of the same set of student in different exams, or repeated sampling from the same units. The test measures whether the average score differs significantly across samples (e.g. exams).  If we observe a large p-value, for example greater than 0.05 or 0.1 then we cannot reject the null hypothesis of identical average scores. If the p-value is smaller than the threshold, e.g. 1%, 5% or 10%, then we reject the null hypothesis of equal averages. Small p-values are associated with large t-statistics.\n",
    "\n",
    "\n",
    "#### null hypothesis -- that 2 related or repeated samples have identical average (expected) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_liwc(df_new,df_old,alpha):\n",
    "    columns=[\"property\",\"mean_value_new\", \"mean_value_old\" , \"p-value\"]\n",
    "    \n",
    "    stat_new = df_new.describe()\n",
    "    mean_vals_new = stat_new.iloc[1]\n",
    "    \n",
    "    stat_old = df_old.describe()\n",
    "    mean_vals_old = stat_old.iloc[1]\n",
    "\n",
    "    \n",
    "    colnames = df_new.columns.values.tolist()\n",
    "    colnames = colnames[1:]\n",
    "    \n",
    "    final_vals = []\n",
    "    \n",
    "    for each_col in colnames:\n",
    "        try:\n",
    "            res = scipy.stats.wilcoxon(df_old[each_col],df_new[each_col])\n",
    "            if(res.pvalue < alpha):\n",
    "                final_vals.append([each_col,mean_vals_new[each_col],mean_vals_old[each_col],res.pvalue])\n",
    "        except:\n",
    "            print(each_col)\n",
    "    print(final_vals)\n",
    "    stat_significant_df = pd.DataFrame(final_vals,columns=columns)\n",
    "    return stat_significant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analyse_liwc(df_new,df_old,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_new['family'],df_old['family'])\n",
    "df.sort_values(by=['p-value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Authentic'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old['Authentic'].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
