{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt \n",
    "# %matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import os\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# import seaborn as sn\n",
    "import csv\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import *\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from pprint import pprint\n",
    "import tweepy\n",
    "import random\n",
    "import emoji\n",
    "import json\n",
    "import re\n",
    "import subprocess\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from dateutil.parser import parse\n",
    "import requests\n",
    "import plotly.graph_objects as go\n",
    "import emoji\n",
    "import demoji\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install demoji\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account behaviour before/after suspension/deletion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Dataset\n",
    "Comparing the set of tweets for each account before and after suspension date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_ids_path = set()\n",
    "def uid_list(root_dir):\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in tqdm(files):\n",
    "            filename = subdir + file\n",
    "            filename = filename.rsplit(\"_\",1)[0]\n",
    "            usr_ids_path.add(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_files(uid):\n",
    "    f1 = uid + \"_old.json\"\n",
    "    f2 = uid + \"_new.json\"\n",
    "    act_id = uid.rsplit(\"/\",1)[1]\n",
    "    \n",
    "    try:\n",
    "        df1 = pd.read_json(f1)\n",
    "        df1['user_id'] = act_id\n",
    "        df2 = pd.read_json(f2)\n",
    "        df2['user_id'] = act_id\n",
    "        return df1,df2\n",
    "    except:\n",
    "        return None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpath = \"/Deleted_Accounts/personality_analysis_before_after/twint_all_unsuspended/2385754142.json\"\n",
    "# with open(fpath, 'r') as f:\n",
    "#     cnt = 0\n",
    "#     for lines in f:\n",
    "#         cnt += 1\n",
    "# #         print(liness.split()[0])\n",
    "#     print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_list(\"../personality_analysis_before_after/undeleted_accounts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(usr_ids_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_create_pd(pd1,pd2):\n",
    "    old_global.append(pd1)\n",
    "    new_global.append(pd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse():\n",
    "    old_global = []\n",
    "    new_global = []\n",
    "    for each in tqdm(usr_ids_path):\n",
    "        dfold,dfnew = finding_files(each)\n",
    "        if(dfold is not None):\n",
    "            old_global.append(dfold)\n",
    "            new_global.append(dfnew)\n",
    "    print(len(old_global) , \"no_of_accounts\")\n",
    "    mt = 0\n",
    "    for each in old_global:\n",
    "        mt = max(mt,each.shape[0])\n",
    "    print(mt,\" old max tweets\")\n",
    "    \n",
    "    mt = 0\n",
    "    for each in new_global:\n",
    "        mt = max(mt,each.shape[0])\n",
    "    print(mt,\" new max tweets\")\n",
    "    \n",
    "    old_global_merge = pd.concat(old_global)\n",
    "    new_global_merge = pd.concat(new_global)\n",
    "    \n",
    "    return old_global,new_global,old_global_merge,new_global_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_global,new_global,old_global_merge,new_global_merge = analyse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = old_global_merge.resample('D', on='timestamp')['has_media'].count()\n",
    "x2 = new_global_merge.resample('D', on='timestamp')['has_media'].count()\n",
    "x1['MA'] = x1.rolling(window=7).mean()\n",
    "x2['MA'] = x2.rolling(window=7).mean()\n",
    "plot(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x1,x2):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.xlabel('Tweet frequency with time')\n",
    "\n",
    "    ax1 = x1['MA'].plot(color='blue', grid=True, label='before-deletion')\n",
    "    ax2 = x2['MA'].plot(color='red', grid=True,  label='after-deletion')\n",
    "\n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "#     h2, l2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "\n",
    "    plt.legend(h1, l1, loc=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x.count().plot(kind = 'line',use_index = 'False',title = 'Before suspended'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_global_merge.resample('H', on='timestamp')['likes']\n",
    "data_ser = y.count().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for val in hr.iteritems(): \n",
    "#     print(val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_details = {\n",
    "\"consumer_key\" : \"\",\n",
    "\"consumer_secret\" : \"\",\n",
    "\"access_token\" : \"-\",\n",
    "\"access_token_secret\" : \"\",\n",
    "\"in_use\" : False,\n",
    "\"rate_limit_hit\" : False,\n",
    "\"error_status\" : False\n",
    "}\n",
    "\n",
    "keys = []\n",
    "keys.append(api_details)\n",
    "\n",
    "api_details = {\n",
    "\"consumer_key\" : \"\",\n",
    "\"consumer_secret\" : \"\",\n",
    "\"access_token\" : \"\",\n",
    "\"access_token_secret\" : \"\",\n",
    "\"in_use\" : False,\n",
    "\"rate_limit_hit\" : False,\n",
    "\"error_status\" : False\n",
    "}\n",
    "\n",
    "keys.append(api_details)\n",
    "\n",
    "api_details = {\n",
    "\"consumer_key\" : \"\",\n",
    "\"consumer_secret\" : \"\",\n",
    "\"access_token\" : \"\",\n",
    "\"access_token_secret\" : \"\",\n",
    "\"in_use\" : False,\n",
    "\"rate_limit_hit\" : False,\n",
    "\"error_status\" : False\n",
    "}\n",
    "\n",
    "keys.append(api_details)\n",
    "\n",
    "api_details = {\n",
    "\"consumer_key\" : \"\",\n",
    "\"consumer_secret\" : \"\",\n",
    "\"access_token\" : \"\",\n",
    "\"access_token_secret\" : \"\",\n",
    "\"in_use\" : False,\n",
    "\"rate_limit_hit\" : False,\n",
    "\"error_status\" : False\n",
    "}\n",
    "\n",
    "keys.append(api_details)\n",
    "\n",
    "def doauth(index):\n",
    "    auth = tweepy.OAuthHandler(keys[index]['consumer_key'], keys[index]['consumer_secret'])\n",
    "    auth.set_access_token(keys[index]['access_token'], keys[index]['access_token_secret'])\n",
    "    api = tweepy.API(auth)\n",
    "    return api\n",
    "\n",
    "idx = 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,val in enumerate(old_global):\n",
    "    old_global[ind] = old_global[ind].sort_values(by='timestamp')\n",
    "    new_global[ind] = new_global[ind].sort_values(by='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_tweets = []\n",
    "first_tweets = []\n",
    "\n",
    "list_users = []\n",
    "\n",
    "for ind,val in enumerate(old_global):\n",
    "    old_pd = old_global[ind]\n",
    "    new_pd = new_global[ind]\n",
    "    top_row = new_pd.iloc[0]\n",
    "    bot_row = old_pd.iloc[-1]\n",
    "            \n",
    "    first_tweets.append(top_row.tweet_id)\n",
    "    last_tweets.append(bot_row.tweet_id)\n",
    "    list_users.append(top_row.user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sus_ids = set()\n",
    "have_both_sus = []\n",
    "for files in os.listdir(\"../personality_analysis/unsuspended_accounts/\"):\n",
    "    uid = files.split(\"_\")[0]\n",
    "    old_len = len(sus_ids)\n",
    "    sus_ids.add(uid)\n",
    "    new_len = len(sus_ids)\n",
    "    if(new_len == old_len):\n",
    "        have_both_sus.append(uid)\n",
    "\n",
    "del_ids = set()\n",
    "have_both_del = []\n",
    "\n",
    "for files in os.listdir(\"../personality_analysis/undeleted_accounts/\"):\n",
    "    uid = files.split(\"_\")[0]\n",
    "    old_len = len(del_ids)\n",
    "    del_ids.add(uid)\n",
    "    new_len = len(del_ids)\n",
    "    if(new_len == old_len):\n",
    "        have_both_del.append(uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_bot_scores(uid_list):\n",
    "    uid_bot_scores = {}\n",
    "    root_p = \"/Alexandria/suspected_user_project/botometer_data/\"\n",
    "    for files in tqdm(os.listdir(root_p)):\n",
    "        with open(root_p + files , 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for each in data:\n",
    "                uid = each['user']['id_str']\n",
    "                for each_id in uid_list:\n",
    "                    if uid == each_id:\n",
    "                        uid_bot_scores[uid] = each['scores']['universal']\n",
    "    return uid_bot_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sus_scores = get_bot_scores(have_both_sus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del_scores = get_bot_scores(have_both_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_by_thresh(dic,thresh):\n",
    "    cnter = 0\n",
    "    for ind,val in dic.items():\n",
    "        if(val > thresh):\n",
    "            cnter += 1\n",
    "    print(cnter)\n",
    "    return cnter / len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( float(sum(sus_scores.values())) / len(sus_scores) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( float(sum(del_scores.values())) / len(del_scores) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psus = remove_by_thresh(sus_scores,0.5)\n",
    "pdel = remove_by_thresh(del_scores,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psus,pdel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_and_compare(last_tweets,first_tweets):\n",
    "    \n",
    "    for ind,val in enumerate(last_tweets):\n",
    "        first_twid = first_tweets[ind]\n",
    "        last_twid = last_tweets[ind]\n",
    "        print(first_twid,last_twid)\n",
    "        api = doauth(0)\n",
    "        try:\n",
    "            p1 = api.get_status(first_twid)\n",
    "            p2 = api.get_status(last_twid)\n",
    "            dic1 = p1.user._json\n",
    "            dic2 = p2.user._json\n",
    "            \n",
    "            fl = 0\n",
    "            for keys in dic1:\n",
    "                if dic1[keys] != dic2[keys]:\n",
    "                    print(keys,dic1[keys],dic2[keys],list_users[ind],first_twid,last_twid,ind)\n",
    "                    fl = 1\n",
    "            if fl:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_and_compare(last_tweets,first_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_global_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_global_merge.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_global_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_global_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparing_pds(pd1,pd2):\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in tqdm(files):\n",
    "            filename = subdir + os.sep + file\n",
    "            df = pd.read_json(filename)\n",
    "            df['user_id'] = file.split(\"_\")[0]\n",
    "            print(df.head())\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "creating_pds(\"../personality_analysis/undeleted_accounts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_global_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linguistic_analysis(root_dir):\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in tqdm(files):\n",
    "            older_tweets = []\n",
    "            newer_tweets = []\n",
    "            filename = subdir + os.sep + file\n",
    "            \n",
    "            percentage_non_english = []\n",
    "            if(file != '1118084130149928960.csv'):\n",
    "                cont\n",
    "            with open(filename,'r') as f:\n",
    "                data = csv.reader(f)\n",
    "                total_count = 0\n",
    "                r = 0\n",
    "                for row in data:\n",
    "                    r+=1\n",
    "                    if r == 1:\n",
    "                        continue  \n",
    "                    try:\n",
    "                        tweet_text = demoji.replace(row[-2])\n",
    "                        print(tweet_text)\n",
    "                        if(isEnglish(tweet_text) == False):\n",
    "                            print(tweet_text,isEnglish(mytweet),\"FFFFFF\")\n",
    "                            total_count += 1\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                perc = (total_count/r)*100\n",
    "                percentage_non_english.append(total_count/r * 100)\n",
    "                \n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_analysis(\"../deleted_accounts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying IBM Personality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result(source_file,target_file,url,apikey):\n",
    "    headers = {\n",
    "        'Content-Type': 'text/plain;charset=utf-8',\n",
    "        'Accept': 'application/json',\n",
    "    }\n",
    "\n",
    "    params = (\n",
    "        ('version', '2017-10-13'),\n",
    "    )\n",
    "\n",
    "    data = open(source_file,'rb').read()\n",
    "    \n",
    "    response = requests.post( url + '/v3/profile?version=2017-10-13&consumption_preferences=true&raw_scores=true', headers=headers, data=data, auth=('apikey', apikey))\n",
    "    dt = response.json()\n",
    "\n",
    "    with open(target_file, 'w') as fp:\n",
    "        json.dump(dt, fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# root_dir = '/Deleted_Accounts/personality_analysis/unsuspended_accounts'\n",
    "# target_dir = '/Deleted_Accounts/personality_analysis/unsuspended_accounts_scores'\n",
    "# api_key = ''\n",
    "# url_val = 'https://api.us-south.personality-insights.watson.cloud.ibm.com/...'\n",
    "\n",
    "root_dir = '/Deleted_Accounts/personality_analysis/undeleted_accounts'\n",
    "target_dir = '/Deleted_Accounts/personality_analysis/undeleted_accounts_scores'\n",
    "\n",
    "\n",
    "\n",
    "api_key = ''\n",
    "url_val = 'https://api.eu-gb.personality-insights.watson.cloud.ibm.com/...'\n",
    "\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    for file in tqdm(files):\n",
    "        filename = subdir + os.sep + file\n",
    "        create_result(filename , target_dir + '/' + file.split('.')[0] + '.json',url_val,api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of people with following schema\n",
    "#  { userid : { 'old_vals' : { 'name_property1' : {child_name : value ,..... } ,  } } , new_vals ...same }\n",
    "## List of 25 properties\n",
    "name_properites = []\n",
    "def get_properties(path):\n",
    "    data = {}\n",
    "    with open(path,'r') as f:\n",
    "        data = json.load(f)\n",
    "        for keys in data['personality']:\n",
    "            mtype = (keys['name'])\n",
    "            print(mtype)\n",
    "            for each in keys['children']:\n",
    "                name_properites.append(each['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_scores = {};\n",
    "\n",
    "def load_vals(path):\n",
    "    data = {}\n",
    "    with open(path,'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    \n",
    "    uid = path.split('/')[-1].split('_')[0]\n",
    "    print(path)\n",
    "    if uid not in user_scores:\n",
    "        user_scores[uid] = {'deleted':{'old_vals':[],'new_vals':[]},'suspended':{'old_vals':[],'new_vals':[] } }  \n",
    "    \n",
    "    try:   \n",
    "        if('old' in path and 'undeleted' in path):\n",
    "            for keys in data['personality']:\n",
    "                mtype = (keys['name'])\n",
    "                for each in keys['children']:\n",
    "                    user_scores[uid]['deleted']['old_vals'].append(each['percentile'])\n",
    "\n",
    "        if('old' in path and 'unsuspended' in path):\n",
    "            for keys in data['personality']:\n",
    "                mtype = (keys['name'])\n",
    "                for each in keys['children']:\n",
    "                    user_scores[uid]['suspended']['old_vals'].append(each['percentile'])\n",
    "\n",
    "        if('new' in path and 'undeleted' in path):\n",
    "            for keys in data['personality']:\n",
    "                mtype = (keys['name'])\n",
    "                for each in keys['children']:\n",
    "                    user_scores[uid]['deleted']['new_vals'].append(each['percentile'])\n",
    "\n",
    "        if('new' in path and 'unsuspended' in path):\n",
    "            for keys in data['personality']:\n",
    "                mtype = (keys['name'])\n",
    "                for each in keys['children']:\n",
    "                    user_scores[uid]['suspended']['new_vals'].append(each['percentile'])\n",
    "    except:\n",
    "        pass\n",
    "                \n",
    "#                 user_scores[uid]['deleted']['old_vals'].append()\n",
    "#                 name_properites.append(each['name'])\n",
    "#     print(user_scores[uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_dir = '/Deleted_Accounts/personality_analysis/unsuspended_accounts_scores'\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    for file in tqdm(files):\n",
    "        filename = subdir + os.sep + file\n",
    "        load_vals(filename)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_be_removed = []\n",
    "for key,value in user_scores.items():\n",
    "    if ( (value['suspended']['old_vals'] == []) or  (value['suspended']['new_vals'] == []) ):\n",
    "        key_to_be_removed.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in key_to_be_removed:\n",
    "    del(user_scores[each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/Deleted_Accounts/personality_analysis/unsuspended_accounts_scores'\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    for file in tqdm(files):\n",
    "        filename = subdir + os.sep + file\n",
    "        get_properties(filename)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_mean_scores(user_scores):\n",
    "    scoring_list_old = []\n",
    "    scoring_list_new = []\n",
    "    \n",
    "    counter = 0\n",
    "    for key,value in user_scores.items():\n",
    "        \n",
    "        counter+=1\n",
    "        p_old = (value['suspended']['old_vals'])\n",
    "        p_new = (value['suspended']['new_vals'])\n",
    "        for ind,val in enumerate(p_old):\n",
    "            if scoring_list_old == []:\n",
    "                scoring_list_old = p_old\n",
    "            else:\n",
    "                scoring_list_old[ind] += val\n",
    "        \n",
    "        for ind,val in enumerate(p_new):\n",
    "            if scoring_list_new == []:\n",
    "                scoring_list_new = p_new\n",
    "            else:\n",
    "                scoring_list_new[ind] += val\n",
    "    \n",
    "    scoring_list_old[:] = [x / counter for x in scoring_list_old]\n",
    "    scoring_list_new[:] = [x / counter for x in scoring_list_new]\n",
    "    \n",
    "    return  scoring_list_old,scoring_list_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_list_old,scoring_list_new = finding_mean_scores(user_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radial_plot(categories,old_vals,new_vals):\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "          r=old_vals,\n",
    "          theta=categories,\n",
    "          fill='toself',\n",
    "          name='Before Suspension'\n",
    "    ))\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "          r=new_vals,\n",
    "          theta=categories,\n",
    "          fill='toself',\n",
    "          name='After Suspension'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "      polar=dict(\n",
    "        radialaxis=dict(\n",
    "          visible=True,\n",
    "          range=[0, 0.8]\n",
    "        )),\n",
    "      showlegend=True\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "radial_plot(name_properites[0:5],scoring_list_old[0:5],scoring_list_new[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
