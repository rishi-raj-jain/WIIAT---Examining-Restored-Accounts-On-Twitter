{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt \n",
    "# %matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import os\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# import seaborn as sn\n",
    "import csv\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import *\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from pprint import pprint\n",
    "import tweepy\n",
    "import random\n",
    "import emoji\n",
    "import json\n",
    "import re\n",
    "import subprocess\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from dateutil.parser import parse\n",
    "import requests\n",
    "import plotly.graph_objects as go\n",
    "import emoji\n",
    "import demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install demoji\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePattern(text, pattern):\n",
    "\n",
    "    r = re.findall(pattern, text)\n",
    "\n",
    "    for i in r:\n",
    "\n",
    "        text = re.sub(i, '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessTweet(tweet_text, only_english=True, stemming=False, remove_more_than=3):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Function to perform various pre-processing steps on  \n",
    "\n",
    "    tweet text\n",
    "\n",
    "\n",
    "\n",
    "    Parameters\n",
    "\n",
    "    ----------\n",
    "\n",
    "    only_english : bool\n",
    "\n",
    "        removes characters from other languages when set to True;\n",
    "\n",
    "        defaults to False\n",
    "\n",
    "    \n",
    "\n",
    "    remove_more_than : int\n",
    "\n",
    "        remove words more than three characters\n",
    "\n",
    "    \n",
    "\n",
    "    stemming : bool\n",
    "\n",
    "        performs stemming if set to True; defaults to False\n",
    "\n",
    "        \n",
    "\n",
    "    tweet_text : str\n",
    "\n",
    "        < 280 character string of text\n",
    "\n",
    "    \n",
    "\n",
    "    Returns\n",
    "\n",
    "    -------\n",
    "\n",
    "    tweet_text\n",
    "\n",
    "        processed tweet\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    tweet_text = tweet_text.lower()\n",
    "\n",
    "    tweet_text = removePattern(tweet_text, \"@[\\w]*\") # remove twitter handles\n",
    "\n",
    "    tweet_text = removePattern(tweet_text, \"&[\\w]*\") # remove &amp\n",
    "\n",
    "    tweet_text = re.sub('[!@$:).;/#,*$?।&…\"]', '', tweet_text) # remove special characters, punctuations\n",
    "\n",
    "    tweet_text = removePattern(tweet_text, \"http[\\w]*\")\n",
    "\n",
    "    if only_english:\n",
    "\n",
    "        tweet_text = tweet_text.encode('ascii', 'ignore').decode('ascii') # remove emojis and other languages\n",
    "\n",
    "    else:\n",
    "\n",
    "        tweet_text = emoji.get_emoji_regexp().sub(u'', tweet_text) # just remove emojis\n",
    "\n",
    "    \n",
    "\n",
    "    if stemming:\n",
    "\n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "        tweet_text = ' '.join([stemmer.stem(word).strip() for word in tweet_text.split() \\\n",
    "\n",
    "                               if len(word)>=remove_more_than and word not in STOPWORDS]) # removing stems, short words and stop words\n",
    "\n",
    "    else:\n",
    "\n",
    "        tweet_text = ' '.join([word.strip() for word in tweet_text.split() \\\n",
    "\n",
    "                               if len(word)>=remove_more_than and word not in STOPWORDS]) # removing short words and stop words\n",
    "    \n",
    "    return tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract average tweets per hour\n",
    "\"\"\"['account_id', 'followers', 'friends', 'favourites', 'account_creation', 'is_verified', 'name', 'screen_name', \n",
    "'description', 'tweet_id', 'tweet_creation', 'tweet_text', 'is_RT']\"\"\"\n",
    "\n",
    "def extract_profile_features(root_dir):\n",
    "    features = {} \n",
    "    ctime = pd.Timestamp.now(tz='UTC')\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in tqdm(files):\n",
    "            filename = subdir + os.sep + file\n",
    "            with open(filename,'r')as f:\n",
    "                data = csv.reader(f)\n",
    "                r = 0\n",
    "                last_tweet = 0\n",
    "                cur_hr = 1.0\n",
    "                avg_list = [] \n",
    "                userid = 0\n",
    "                words = \"\"\n",
    "                usr_features = {}\n",
    "                for row in data:\n",
    "                    r+=1\n",
    "                    if r == 1:\n",
    "                        continue  \n",
    "                    #first tweet (the latest)\n",
    "                    try:\n",
    "                        otime = pd.to_datetime(row[10])\n",
    "                        cur_time = (ctime-otime).total_seconds()\n",
    "                        if abs(cur_time-last_tweet) <= 3600:\n",
    "                            cur_hr += 1.0\n",
    "                        else:\n",
    "                            last_tweet = cur_time\n",
    "                            avg_list.append(cur_hr)\n",
    "                            cur_hr = 1.0\n",
    "                        if r == 2:\n",
    "                            userid = row[0]\n",
    "                            usr_features['followers'] = row[1]\n",
    "                            usr_features['creation_date'] = row[4]\n",
    "                            usr_features['friends'] = row[2]\n",
    "                            usr_features['favourites'] = row[3]\n",
    "                            usr_features['friend_to_follow'] = float(row[2])/(float(row[1])+1)\n",
    "                            usr_features['name_len'] = len(row[6])\n",
    "                            usr_features['bio_len'] = len(row[8])\n",
    "                            usr_features['scr_len'] = len(row[7])\n",
    "                            usr_features['scr_len_words'] = len(row[7].strip(' ').split(' '))\n",
    "                            usr_features['bio_len_words'] = len(row[8].strip(' ').split(' '))\n",
    "                            usr_features['tweet'] = preprocessTweet(row[-2])\n",
    "                        if(len(row) == 13):\n",
    "                            usr_features['tweet'] += preprocessTweet(row[-2])\n",
    "                    except:\n",
    "                        pass\n",
    "#                         print(row)\n",
    "#                         print(file)\n",
    "\n",
    "\n",
    "                usr_features['no_of_tweets'] = r - 1\n",
    "                if len(avg_list) == 0:\n",
    "                    avg_list.append(0)\n",
    "                usr_features['avg_tweets'] = sum(avg_list)/(len(avg_list))\n",
    "#                 print(len(usr_features))\n",
    "                features[userid] = usr_features\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deleted_account_feature = extract_profile_features(\"../deleted_accounts\")\n",
    "suspended_account_feature = extract_profile_features(\"../suspended_accounts\")\n",
    "normal_account_feature = extract_profile_features(\"../normal_accounts\")\n",
    "\n",
    "df1 = pd.DataFrame.from_dict(deleted_account_feature).T\n",
    "df2 = pd.DataFrame.from_dict(suspended_account_feature).T\n",
    "df3 = pd.DataFrame.from_dict(normal_account_feature).T\n",
    "\n",
    "df1 = df1.dropna()\n",
    "df2 = df2.dropna()\n",
    "df3 = df3.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"deleted_account_features.csv\")\n",
    "df2.to_csv(\"suspended_account_features.csv\")\n",
    "df3.to_csv(\"normal_account_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"deleted_account_features.csv\")\n",
    "df2 = pd.read_csv(\"suspended_account_features.csv\")\n",
    "df3 = pd.read_csv(\"normal_account_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['creation_date'] = pd.to_datetime(df1['creation_date'])\n",
    "df2['creation_date'] = pd.to_datetime(df2['creation_date'])\n",
    "df3['creation_date'] = pd.to_datetime(df3['creation_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid.inset_locator import inset_axes\n",
    "\n",
    "def plot_creation_date(dt_col,label,x1=1,x2=1,y1=1,y2=1):\n",
    "    ser = pd.Series(1, index= dt_col).resample('M', how='count')\n",
    "    ser.index = ser.index.to_period('M')\n",
    "    tot_counts = ser.sum()\n",
    "    ser /= tot_counts\n",
    "#     print(ser)\n",
    "    ax = ser.plot()\n",
    "    ax.set(xlabel=\"Creation_Date\", ylabel=\"Percentage of Accouts Created\")\n",
    "    ax.set_title(label)\n",
    "    axins = inset_axes(ax, 2,1.5 , loc=2,bbox_to_anchor= (0.2, 0.85),bbox_transform= ax.figure.transFigure) # no zoom\n",
    "#     axins = zoomed_inset_axes(ax, 2.0, loc=2) # zoom-factor: 2.5, location: upper-left\n",
    "    axins = ser.plot(marker='o')\n",
    "    axins.set(xlabel=\"\")\n",
    "#    x1, x2, y1, y2 = 2018, 60, 3.7, 4.6 # specify the limits\n",
    "    axins.set_xlim(x1, x2) # apply the x-limits\n",
    "#     axins.set_ylim(y1, y2) # apply the y-limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_creation_date(df1['creation_date'], \"deleted_accounts\", x1 = '2019-02',x2 = '2019-06', y1 = 0.06, y2 = 0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_creation_date(df2['creation_date'], \"suspended_accounts\", x1= '2019-02', x2= '2019-06', y1= 0.05, y2= 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_creation_date(df3['creation_date'],\"normal_accounts\",x1 = '2019-02',x2 = '2019-06',y1 = 0.004,y2 = 0.06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account behaviour before/after suspension/deletion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Dataset\n",
    "Identifying a set of tweets for each account before and after suspension date. \n",
    "\n",
    "### Step-Wise method for the same :-\n",
    "-  Convert userid to username \n",
    "-  Query username between start-date (29th April - 2 months below suspension date)  and end-date (8th December - 2 months after suspension was lifted).\n",
    "-  Store json files for each user as userid_old.json and userid_new.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_details = {\n",
    "\"consumer_key\" : \"\",\n",
    "\"consumer_secret\" : \"\",\n",
    "\"access_token\" : \"\",\n",
    "\"access_token_secret\" : \"\",\n",
    "\"in_use\" : False,\n",
    "\"rate_limit_hit\" : False,\n",
    "\"error_status\" : False\n",
    "}\n",
    "\n",
    "keys = []\n",
    "keys.append(api_details)\n",
    "\n",
    "api_details = {\n",
    "\"consumer_key\" : \"\",\n",
    "\"consumer_secret\" : \"\",\n",
    "\"access_token\" : \"\",\n",
    "\"access_token_secret\" : \"\",\n",
    "\"in_use\" : False,\n",
    "\"rate_limit_hit\" : False,\n",
    "\"error_status\" : False\n",
    "}\n",
    "\n",
    "keys.append(api_details)\n",
    "\n",
    "api_details = {\n",
    "\"consumer_key\" : \"\",\n",
    "\"consumer_secret\" : \"\",\n",
    "\"access_token\" : \"\",\n",
    "\"access_token_secret\" : \"\",\n",
    "\"in_use\" : False,\n",
    "\"rate_limit_hit\" : False,\n",
    "\"error_status\" : False\n",
    "}\n",
    "\n",
    "keys.append(api_details)\n",
    "\n",
    "api_details = {\n",
    "\"consumer_key\" : \"\",\n",
    "\"consumer_secret\" : \"\",\n",
    "\"access_token\" : \"\",\n",
    "\"access_token_secret\" : \"\",\n",
    "\"in_use\" : False,\n",
    "\"rate_limit_hit\" : False,\n",
    "\"error_status\" : False\n",
    "}\n",
    "\n",
    "keys.append(api_details)\n",
    "\n",
    "def doauth(index):\n",
    "    auth = tweepy.OAuthHandler(keys[index]['consumer_key'], keys[index]['consumer_secret'])\n",
    "    auth.set_access_token(keys[index]['access_token'], keys[index]['access_token_secret'])\n",
    "    api = tweepy.API(auth)\n",
    "    return api\n",
    "\n",
    "idx = 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userid_to_username(userid):\n",
    "    try:\n",
    "        api = doauth(0)\n",
    "        user = api.get_user(userid)\n",
    "        return user.screen_name\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_tweets(uid,screen_name,begin_date,end_date,folder,file):\n",
    "    \n",
    "    params = \"twitterscraper 'from:\" + screen_name + \"' -bd \" + begin_date + \" -ed \" + end_date + \" -p 15\" + \" -o \" + \"../personality_analysis/\" + str(folder) + \"/\" + str(uid) + file + \".json\"\n",
    "    print(params)\n",
    "    try:\n",
    "        op = subprocess.run(params,shell=True)\n",
    "    except:\n",
    "        print(\"QUERY_FAIL\",\"WGGGY\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def extract_accounts(root_dir):\n",
    "    old_tweets_begin = \"2019-04-29\"\n",
    "    old_tweets_end = \"2019-06-29\"\n",
    "    new_tweets_begin = \"2019-10-08\"\n",
    "    new_tweets_end = \"2019-12-08\"\n",
    "    ftype = \"undeleted_accounts\"\n",
    "    \n",
    "    if(\"unsuspended\" in root_dir):\n",
    "        ftype = \"unsuspended_accounts\"\n",
    "        \n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in tqdm(files):\n",
    "            uid = file.split(\".\")[0]\n",
    "            screen_name = userid_to_username(uid)\n",
    "            if(screen_name == -1):\n",
    "                print(\"WRONG\")\n",
    "                continue\n",
    "                \n",
    "            fpath1 = \"../personality_analysis/\" + str(ftype) + \"/\" + str(uid) +  \"_old.json\"\n",
    "            fpath2 = \"../personality_analysis/\" + str(ftype) + \"/\" + str(uid) +  \"_new.json\"\n",
    "            \n",
    "            if (os.path.isfile(fpath1)):\n",
    "                continue\n",
    "            if (os.path.isfile(fpath2)):\n",
    "                continue\n",
    "                \n",
    "            \n",
    "            \n",
    "            try:\n",
    "                query_tweets(uid,screen_name,old_tweets_begin,old_tweets_end,ftype,\"_old\")\n",
    "            except:\n",
    "                print(\"OLD_QUERY_FAIL\",uid)\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                query_tweets(uid,screen_name,new_tweets_begin,new_tweets_end,ftype,\"_new\")\n",
    "            except:\n",
    "                print(\"NEW_QUERY_FAIL\",uid)\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                fpath = \"../personality_analysis/\" + str(ftype) + \"/\" + str(uid) +  \"_old.json\"\n",
    "                df = pd.read_json(fpath)\n",
    "                df.drop(['screen_name','text_html','timestamp_epochs','tweet_url','user_id','username'], axis=1, inplace=True)\n",
    "                df.to_json(fpath)\n",
    "            except:\n",
    "                print(\"OLD_WRITE_FAIL\",uid)\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                fpath = \"../personality_analysis/\" + str(ftype) + \"/\" + str(uid) +  \"_new.json\"\n",
    "                df = pd.read_json(fpath)\n",
    "                df.drop(['screen_name','text_html','timestamp_epochs','tweet_url','user_id','username'], axis=1, inplace=True)\n",
    "                df.to_json(fpath)\n",
    "            except:\n",
    "                print(\"NEW_WRITE_FAIL\",uid)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_accounts(\"../undeleted_accounts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_accounts(\"../unsuspended_accounts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"['account_id', 'followers', 'friends', 'favourites', 'account_creation', 'is_verified', 'name', 'screen_name', \n",
    "'description', 'tweet_id', 'tweet_creation', 'tweet_text', 'is_RT']\"\"\"\n",
    "\n",
    "compare_date = parse(\"2019-10-09 00:00:00+00:00\")\n",
    "def seperate_tweets_accounts(root_dir,target_dir):\n",
    "    cnt = 0\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in tqdm(files):\n",
    "#             print(file)\n",
    "            if(file != '18732540.csv'):\n",
    "                continue\n",
    "            \n",
    "            older_tweets = []\n",
    "            newer_tweets = []\n",
    "            filename = subdir + os.sep + file\n",
    "            \n",
    "            with open(filename,'r')as f:\n",
    "                data = csv.reader(f)\n",
    "                r = 0\n",
    "                for row in data:\n",
    "                    r+=1\n",
    "                    if r == 1:\n",
    "                        older_tweets.append(row)\n",
    "                        newer_tweets.append(row)\n",
    "                        continue  \n",
    "                    try:\n",
    "                        otime = parse(row[10])\n",
    "                        if(otime < compare_date):\n",
    "                            print(otime,row[-4])\n",
    "                            older_tweets.append(row)\n",
    "                        else:\n",
    "                            newer_tweets.append(row)\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            limit = 100\n",
    "#             if ( len(older_tweets) >= limit and len(newer_tweets) >= limit ) :\n",
    "#                 fname1 = target_dir+file.split(\".\")[0]+ \"_old.csv\"\n",
    "#                 fname2 = target_dir+file.split(\".\")[0]+ \"_new.csv\"\n",
    "\n",
    "#                 with open(fname1,'a+') as ff:\n",
    "#                     for ind,val in enumerate(older_tweets):\n",
    "#                         if(ind == 0):\n",
    "#                             continue\n",
    "#                         ff.write(older_tweets[ind][-2] + '\\n')\n",
    "                        \n",
    "#                 with open(fname2,'a+') as ff:\n",
    "#                     for ind,val in enumerate(newer_tweets):\n",
    "#                         if(ind == 0):\n",
    "#                             continue\n",
    "#                         ff.write(newer_tweets[ind][-2] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seperate_tweets_accounts('../unsuspended_accounts/' , '../personality_analysis/unsuspended_accounts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seperate_tweets_accounts('../undeleted_accounts/' , '../personality_analysis/undeleted_accounts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linguistic_analysis(root_dir):\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in tqdm(files):\n",
    "            older_tweets = []\n",
    "            newer_tweets = []\n",
    "            filename = subdir + os.sep + file\n",
    "            \n",
    "            percentage_non_english = []\n",
    "            if(file != '1118084130149928960.csv'):\n",
    "                cont\n",
    "            with open(filename,'r') as f:\n",
    "                data = csv.reader(f)\n",
    "                total_count = 0\n",
    "                r = 0\n",
    "                for row in data:\n",
    "                    r+=1\n",
    "                    if r == 1:\n",
    "                        continue  \n",
    "                    try:\n",
    "                        tweet_text = demoji.replace(row[-2])\n",
    "                        print(tweet_text)\n",
    "                        if(isEnglish(tweet_text) == False):\n",
    "                            print(tweet_text,isEnglish(mytweet),\"FFFFFF\")\n",
    "                            total_count += 1\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                perc = (total_count/r)*100\n",
    "                percentage_non_english.append(total_count/r * 100)\n",
    "                \n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_analysis(\"../deleted_accounts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying IBM Personality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result(source_file,target_file,url,apikey):\n",
    "    headers = {\n",
    "        'Content-Type': 'text/plain;charset=utf-8',\n",
    "        'Accept': 'application/json',\n",
    "    }\n",
    "\n",
    "    params = (\n",
    "        ('version', '2017-10-13'),\n",
    "    )\n",
    "\n",
    "    data = open(source_file,'rb').read()\n",
    "    \n",
    "    response = requests.post( url + '/v3/profile?version=2017-10-13&consumption_preferences=true&raw_scores=true', headers=headers, data=data, auth=('apikey', apikey))\n",
    "    dt = response.json()\n",
    "\n",
    "    with open(target_file, 'w') as fp:\n",
    "        json.dump(dt, fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# root_dir = '/Deleted_Accounts/personality_analysis/unsuspended_accounts'\n",
    "# target_dir = '/Deleted_Accounts/personality_analysis/unsuspended_accounts_scores'\n",
    "# api_key = ''\n",
    "# url_val = 'https://api.us-south.personality-insights.watson.cloud.ibm.com/...'\n",
    "\n",
    "root_dir = '/Deleted_Accounts/personality_analysis/undeleted_accounts'\n",
    "target_dir = '/Deleted_Accounts/personality_analysis/undeleted_accounts_scores'\n",
    "\n",
    "\n",
    "\n",
    "api_key = ''\n",
    "url_val = # 'https://api.eu-gb.personality-insights.watson.cloud.ibm.com/...'\n",
    "\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    for file in tqdm(files):\n",
    "        filename = subdir + os.sep + file\n",
    "        create_result(filename , target_dir + '/' + file.split('.')[0] + '.json',url_val,api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of people with following schema\n",
    "#  { userid : { 'old_vals' : { 'name_property1' : {child_name : value ,..... } ,  } } , new_vals ...same }\n",
    "## List of 25 properties\n",
    "name_properites = []\n",
    "def get_properties(path):\n",
    "    data = {}\n",
    "    with open(path,'r') as f:\n",
    "        data = json.load(f)\n",
    "        for keys in data['personality']:\n",
    "            mtype = (keys['name'])\n",
    "            print(mtype)\n",
    "            for each in keys['children']:\n",
    "                name_properites.append(each['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_scores = {};\n",
    "\n",
    "def load_vals(path):\n",
    "    data = {}\n",
    "    with open(path,'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    \n",
    "    uid = path.split('/')[-1].split('_')[0]\n",
    "    print(path)\n",
    "    if uid not in user_scores:\n",
    "        user_scores[uid] = {'deleted':{'old_vals':[],'new_vals':[]},'suspended':{'old_vals':[],'new_vals':[] } }  \n",
    "    \n",
    "    try:   \n",
    "        if('old' in path and 'undeleted' in path):\n",
    "            for keys in data['personality']:\n",
    "                mtype = (keys['name'])\n",
    "                for each in keys['children']:\n",
    "                    user_scores[uid]['deleted']['old_vals'].append(each['percentile'])\n",
    "\n",
    "        if('old' in path and 'unsuspended' in path):\n",
    "            for keys in data['personality']:\n",
    "                mtype = (keys['name'])\n",
    "                for each in keys['children']:\n",
    "                    user_scores[uid]['suspended']['old_vals'].append(each['percentile'])\n",
    "\n",
    "        if('new' in path and 'undeleted' in path):\n",
    "            for keys in data['personality']:\n",
    "                mtype = (keys['name'])\n",
    "                for each in keys['children']:\n",
    "                    user_scores[uid]['deleted']['new_vals'].append(each['percentile'])\n",
    "\n",
    "        if('new' in path and 'unsuspended' in path):\n",
    "            for keys in data['personality']:\n",
    "                mtype = (keys['name'])\n",
    "                for each in keys['children']:\n",
    "                    user_scores[uid]['suspended']['new_vals'].append(each['percentile'])\n",
    "    except:\n",
    "        pass\n",
    "                \n",
    "#                 user_scores[uid]['deleted']['old_vals'].append()\n",
    "#                 name_properites.append(each['name'])\n",
    "#     print(user_scores[uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = '/Deleted_Accounts/personality_analysis/undeleted_accounts_scores'\n",
    "\n",
    "# for subdir, dirs, files in os.walk(root_dir):\n",
    "#     for file in tqdm(files):\n",
    "#         filename = subdir + os.sep + file\n",
    "#         load_vals(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_dir = '/Deleted_Accounts/personality_analysis/unsuspended_accounts_scores'\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    for file in tqdm(files):\n",
    "        filename = subdir + os.sep + file\n",
    "        load_vals(filename)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_be_removed = []\n",
    "for key,value in user_scores.items():\n",
    "    if ( (value['suspended']['old_vals'] == []) or  (value['suspended']['new_vals'] == []) ):\n",
    "        key_to_be_removed.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in key_to_be_removed:\n",
    "    del(user_scores[each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/Deleted_Accounts/personality_analysis/unsuspended_accounts_scores'\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    for file in tqdm(files):\n",
    "        filename = subdir + os.sep + file\n",
    "        get_properties(filename)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_mean_scores(user_scores):\n",
    "    scoring_list_old = []\n",
    "    scoring_list_new = []\n",
    "    \n",
    "    counter = 0\n",
    "    for key,value in user_scores.items():\n",
    "        \n",
    "        counter+=1\n",
    "        p_old = (value['suspended']['old_vals'])\n",
    "        p_new = (value['suspended']['new_vals'])\n",
    "        for ind,val in enumerate(p_old):\n",
    "            if scoring_list_old == []:\n",
    "                scoring_list_old = p_old\n",
    "            else:\n",
    "                scoring_list_old[ind] += val\n",
    "        \n",
    "        for ind,val in enumerate(p_new):\n",
    "            if scoring_list_new == []:\n",
    "                scoring_list_new = p_new\n",
    "            else:\n",
    "                scoring_list_new[ind] += val\n",
    "    \n",
    "    scoring_list_old[:] = [x / counter for x in scoring_list_old]\n",
    "    scoring_list_new[:] = [x / counter for x in scoring_list_new]\n",
    "    \n",
    "    return  scoring_list_old,scoring_list_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_list_old,scoring_list_new = finding_mean_scores(user_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radial_plot(categories,old_vals,new_vals):\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "          r=old_vals,\n",
    "          theta=categories,\n",
    "          fill='toself',\n",
    "          name='Before Suspension'\n",
    "    ))\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "          r=new_vals,\n",
    "          theta=categories,\n",
    "          fill='toself',\n",
    "          name='After Suspension'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "      polar=dict(\n",
    "        radialaxis=dict(\n",
    "          visible=True,\n",
    "          range=[0, 0.8]\n",
    "        )),\n",
    "      showlegend=True\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "radial_plot(name_properites[0:5],scoring_list_old[0:5],scoring_list_new[0:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
