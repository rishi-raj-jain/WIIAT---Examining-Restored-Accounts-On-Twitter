{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory():\n",
    "    import re\n",
    "    status = open('/proc/self/status').read()\n",
    "    memuse = int(re.findall('VmRSS:\\s*(\\d+)\\skB', status)[0]) / 1024 ** 2\n",
    "    print('Resident memory: %.2f GB' % memuse)\n",
    "    return memuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def loadPickle(name):\n",
    "    return pd.read_pickle(f'../data/{name}')\n",
    "def loadTxt(name):\n",
    "    return [i.strip() for i in open(f'../data/{name}', 'r').readlines()]\n",
    "def dumpPickle(df, name):\n",
    "    df.to_pickle(f'../data/{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizing N, S & R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of normal users\n",
    "Normal= loadPickle('metadata_normal_dump.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "S= loadPickle('metadata_suspended_dump.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[Regular Users] - No. of Tweets: {len(Normal.index)}, No. of Users: {len(Normal.groupby(\"user.id_str\"))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the set of restored users\n",
    "restoredUsersList= set()\n",
    "for i in loadTxt('unsuspended_8_Oct.txt'):\n",
    "    restoredUsersList.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restored users\n",
    "Restored= S[S['user.id_str'].isin(restoredUsersList)==True].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[Restored Users] - No. of Tweets: {len(Restored.index)}, No. of Users: {len(Restored.groupby(\"user.id_str\"))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suspended users\n",
    "Suspended= S[S['user.id_str'].isin(restoredUsersList)==False].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[Suspended Users] - No. of Tweets: {len(Suspended.index)}, No. of Users: {len(Suspended.groupby(\"user.id_str\"))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the type of user\n",
    "Normal['typeOfUser']= 'Normal'\n",
    "Restored['typeOfUser']= 'Restored'\n",
    "Suspended['typeOfUser']= 'Suspended'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular & filtering as to the common users\n",
    "RegB= loadPickle('metadata_regular_before.pkl')\n",
    "RegA= loadPickle('metadata_regular_after.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the common users\n",
    "regularBefore= RegB[RegB['user.id_str'].isin(RegA['user.id_str'])].copy()\n",
    "regularAfter= RegA[RegA['user.id_str'].isin(RegB['user.id_str'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[regularBefore Users] - No. of Tweets: {len(regularBefore.index)}, No. of Users: {len(regularBefore.groupby(\"user.id_str\"))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[regularAfter Users] - No. of Tweets: {len(regularAfter.index)}, No. of Users: {len(regularAfter.groupby(\"user.id_str\"))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the type of user\n",
    "regularBefore['typeOfUser']= 'regularBefore'\n",
    "regularAfter['typeOfUser']= 'regularAfter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restored & filtering as to the common users\n",
    "ResB= loadPickle('metadata_restored_before.pkl')\n",
    "ResA= loadPickle('metadata_restored_after.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the pickles\n",
    "restoredBefore= ResB[ResB['user.id_str'].isin(ResA['user.id_str'])==True].copy()\n",
    "restoredAfter= ResA[ResA['user.id_str'].isin(ResB['user.id_str'])==True].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[restoredBefore Users] - No. of Tweets: {len(restoredBefore.index)}, No. of Users: {len(restoredBefore.groupby(\"user.id_str\"))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[restoredAfter Users] - No. of Tweets: {len(restoredAfter.index)}, No. of Users: {len(restoredAfter.groupby(\"user.id_str\"))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the type of user\n",
    "restoredBefore['typeOfUser']= 'restoredBefore'\n",
    "restoredAfter['typeOfUser']= 'restoredAfter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp= []\n",
    "# tempName= []\n",
    "pklDict= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the type of user attribute for merging & differentiating for CDFs\n",
    "# temp.append(Normal)\n",
    "# temp.append(Restored)\n",
    "# temp.append(Suspended)\n",
    "# tempName.append('Normal')\n",
    "# tempName.append('Restored')\n",
    "# tempName.append('Suspended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the pickles for hashtags\n",
    "# pklDict['Normal']= Normal.loc[:, ('entities.hashtags', 'user.id_str')]\n",
    "# pklDict['Restored']= Restored.loc[:, ('entities.hashtags', 'user.id_str')]\n",
    "# pklDict['Suspended']= Suspended.loc[:, ('entities.hashtags', 'user.id_str')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the pickles for text\n",
    "pklDict['Normal']= Normal.loc[:, ('full_text', 'user.id_str')]\n",
    "pklDict['Restored']= Restored.loc[:, ('full_text', 'user.id_str')]\n",
    "pklDict['Suspended']= Suspended.loc[:, ('full_text', 'user.id_str')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the type of user attribute for merging & differentiating for CDFs\n",
    "# temp.append(regularBefore)\n",
    "# temp.append(regularAfter)\n",
    "# tempName.append('regularBefore')\n",
    "# tempName.append('regularAfter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the pickles for hashtags\n",
    "# pklDict['regularBefore']= regularBefore.loc[:, ('entities.hashtags', 'user.id_str')]\n",
    "# pklDict['regularAfter']= regularAfter.loc[:, ('entities.hashtags', 'user.id_str')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the pickles for text\n",
    "pklDict['regularBefore']= regularBefore.loc[:, ('full_text', 'user.id_str')]\n",
    "pklDict['regularAfter']= regularAfter.loc[:, ('full_text', 'user.id_str')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the type of user attribute for merging & differentiating for CDFs\n",
    "# temp.append(restoredBefore)\n",
    "# temp.append(restoredAfter)\n",
    "# tempName.append('restoredBefore')\n",
    "# tempName.append('restoredAfter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the pickles for hashtags\n",
    "# pklDict['restoredBefore']= restoredBefore.loc[:, ('entities.hashtags', 'user.id_str')]\n",
    "# pklDict['restoredAfter']= restoredAfter.loc[:, ('entities.hashtags', 'user.id_str')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the pickles for text\n",
    "pklDict['restoredBefore']= restoredBefore.loc[:, ('full_text', 'user.id_str')]\n",
    "pklDict['restoredAfter']= restoredAfter.loc[:, ('full_text', 'user.id_str')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free the memory \n",
    "Normal= None\n",
    "Restored= None\n",
    "Suspended= None\n",
    "RegB= None\n",
    "RegA= None\n",
    "regularBefore= None\n",
    "regularAfter= None\n",
    "ResB= None\n",
    "ResA= None\n",
    "restoredBefore= None\n",
    "restoredAfter= None\n",
    "temp= None\n",
    "tempName= None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-palmer",
   "metadata": {},
   "source": [
    "###### Hashtag Analysis ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashDictNor= {}\n",
    "hashtagNormal= pklDict['Normal']['entities.hashtags']\n",
    "\n",
    "hashDictSus= {}\n",
    "hashtagSuspended= pklDict['Suspended']['entities.hashtags']\n",
    "\n",
    "hashDictRes= {}\n",
    "hashtagRestored= pklDict['Restored']['entities.hashtags']\n",
    "\n",
    "countAtLeastOneNormal, countAtLeastOneSuspended, countAtLeastOneRestored= 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hashtagNormal:\n",
    "    if len(i)>0:\n",
    "        countAtLeastOneNormal+=1\n",
    "        for j in i:\n",
    "            hashTag= j['text']\n",
    "            hashDictNor[hashTag]= 1 if hashDictNor.get(hashTag) is None else hashDictNor[hashTag] + 1\n",
    "\n",
    "for i in hashtagSuspended:\n",
    "    if len(i)>0:\n",
    "        countAtLeastOneSuspended+=1\n",
    "        for j in i:\n",
    "            hashTag= j['text']\n",
    "            hashDictSus[hashTag]= 1 if hashDictSus.get(hashTag) is None else hashDictSus[hashTag] + 1\n",
    "            \n",
    "for i in hashtagRestored:\n",
    "    if len(i)>0:\n",
    "        countAtLeastOneRestored+=1\n",
    "        for j in i:\n",
    "            hashTag= j['text']\n",
    "            hashDictRes[hashTag]= 1 if hashDictRes.get(hashTag) is None else hashDictRes[hashTag] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashesNor= {k: v for k, v in sorted(hashDictNor.items(), reverse= True, key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashesSus= {k: v for k, v in sorted(hashDictSus.items(), reverse= True, key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashesRes= {k: v for k, v in sorted(hashDictRes.items(), reverse= True, key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashOverall= {}\n",
    "\n",
    "for i in hashesNor:\n",
    "    if hashOverall.get(i) is not None:\n",
    "        hashOverall[i].append({'Normal': hashesNor[i]})\n",
    "    else:\n",
    "        hashOverall[i]= [{'Normal': hashesNor[i]}]\n",
    "\n",
    "for i in hashesSus:\n",
    "    if hashOverall.get(i) is not None:\n",
    "        hashOverall[i].append({'Suspended': hashesSus[i]})\n",
    "    else:\n",
    "        hashOverall[i]= [{'Suspended': hashesSus[i]}]\n",
    "\n",
    "for i in hashesRes:\n",
    "    if hashOverall.get(i) is not None:\n",
    "        hashOverall[i].append({'Restored': hashesRes[i]})\n",
    "    else:\n",
    "        hashOverall[i]= [{'Restored': hashesRes[i]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "T= {'hashTag': [], 'typeUser': [], 'count': []}\n",
    "\n",
    "for i in hashOverall:\n",
    "    if len(hashOverall[i])==3:\n",
    "        temp= ['Normal', 'Suspended', 'Restored']\n",
    "        for j in range(len(hashOverall[i])):\n",
    "            T['hashTag'].append(i)\n",
    "            T['typeUser'].append(temp[j])\n",
    "            T['count'].append(hashOverall[i][j][temp[j]])            \n",
    "\n",
    "T= pd.DataFrame.from_dict(T)\n",
    "T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(T.sort_values(by= 'count'))\n",
    "print(countAtLeastOneNormal, countAtLeastOneSuspended, countAtLeastOneRestored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "sns.ecdfplot(\n",
    "    data= T[T['count']>10],\n",
    "    x= 'count',\n",
    "    hue= 'typeUser',\n",
    "    log_scale= True,\n",
    "    stat= 'proportion'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.ecdfplot(\n",
    "    data= T[T['count']>=10**3],\n",
    "    x= 'count',\n",
    "    hue= 'typeUser',\n",
    "    log_scale= True,\n",
    "    stat= 'proportion'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-winner",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.ecdfplot(\n",
    "    data= T[T['count']>=10**4],\n",
    "    x= 'count',\n",
    "    hue= 'typeUser',\n",
    "    log_scale= True,\n",
    "    stat= 'proportion'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "T3= {}\n",
    "for i, r in T[T['typeUser']=='Normal'].iterrows():\n",
    "    T3[r['hashTag']]= r['count']\n",
    "for i, r in T[T['typeUser']=='Suspended'].iterrows():\n",
    "    T3[r['hashTag']]/= r['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in sorted(T3.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.bar(T.sort_values(by= 'count', ascending= False), x= \"hashTag\", y=\"count\", color='typeUser', title=\"HashTag Usage Comparison\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-optimization",
   "metadata": {},
   "source": [
    "###### Network Analysis ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, csv, pprint, json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from networkx.readwrite import json_graph\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-prisoner",
   "metadata": {},
   "source": [
    "###### Pro vs Anti BJP ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "TowardsBJP= [\n",
    "    'MainBhiChowkidar',\n",
    "    'NamoAgain',\n",
    "    'IndiaWantsModiAgain',\n",
    "    'PhirEkBaarModiSarkar',\n",
    "    'ModiHaiToMumkinHai',\n",
    "    'ChowkidarPhirSe',\n",
    "    'DeshModiKeSaath',\n",
    "    'MannKiBaat',\n",
    "    'ModiOnceMore',\n",
    "    'ModiHiAayega',\n",
    "    'AayegaToModiHi',\n",
    "    'चौकीदार'\n",
    "]\n",
    "AgainstBJP= [\n",
    "    'ModiMuktBharat',\n",
    "    'ChowkidaroKaScam',\n",
    "    'ChowkidarChorHai',\n",
    "    'GoBackModi',\n",
    "    'MainBhiBerozgar',\n",
    "    'GoBackModi',\n",
    "    'GoBackSadistModi',\n",
    "    'JantaMaafNahiKaregi',\n",
    "    'ManmohanSingh',\n",
    "    'ShivSena',\n",
    "    'ModiEkDisaster',\n",
    "    'ModiIsAMistake'\n",
    "]\n",
    "allHashes= TowardsBJP + AgainstBJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashGraphs = nx.Graph()\n",
    "\n",
    "\"\"\"TODO: Add edges to the graph\"\"\"\n",
    "def addToGraph(src, dest, Type, hashType):\n",
    "    global hashGraphs\n",
    "    if hashGraphs.has_edge(src, dest):\n",
    "        hashGraphs[src][dest][\"weight\"] += 1   \n",
    "    else:\n",
    "        hashGraphs.add_edge(src, dest, weight= 1)\n",
    "        hashGraphs[src][dest]['type']= Type\n",
    "        hashGraphs[src][dest]['hashType']= hashType\n",
    "        hashGraphs[src][dest]['cluster']= allHashes.index(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add the normal hashtags to the graph\n",
    "for i in pklDict['Normal'][['entities.hashtags', 'user.id_str']].iterrows():\n",
    "    user= i[1].iloc[1]\n",
    "    hashes= i[1].iloc[0]\n",
    "    for j in hashes:\n",
    "        k= j['text']\n",
    "        if (k in TowardsBJP) or (k in AgainstBJP):\n",
    "            addToGraph(src= user, dest= k, Type= 'Normal', hashType= (k in TowardsBJP)) #hashType would signify that if the hashtag has not been used for BJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add the suspended hashtags to the graph\n",
    "for i in pklDict['Suspended'][['entities.hashtags', 'user.id_str']].iterrows():\n",
    "    user= i[1].iloc[1]\n",
    "    hashes= i[1].iloc[0]\n",
    "    for j in hashes:\n",
    "        k= j['text']\n",
    "        if (k in TowardsBJP) or (k in AgainstBJP):\n",
    "            addToGraph(src= user, dest= k, Type= 'Suspended', hashType= (k in TowardsBJP)) #hashType would signify that if the hashtag has not been used for BJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add the restored hashtags to the graph\n",
    "for i in pklDict['Restored'][['entities.hashtags', 'user.id_str']].iterrows():\n",
    "    user= i[1].iloc[1]\n",
    "    hashes= i[1].iloc[0]\n",
    "    for j in hashes:\n",
    "        k= j['text']\n",
    "        if (k in TowardsBJP) or (k in AgainstBJP):\n",
    "            addToGraph(src= user, dest= k, Type= 'Restored', hashType= (k in TowardsBJP)) #hashType would signify that if the hashtag has not been used for BJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write to CSV\n",
    "def writeToCSV():\n",
    "    with open(\"./graph_regenerated.csv\", \"w\", newline=\"\") as fileSave:\n",
    "        global hashGraphs\n",
    "        fileSave.truncate(0)\n",
    "        writer = csv.writer(fileSave)\n",
    "        writer.writerow([\"Source\", \"Target\", \"Weight\", \"Type\", \"HashType\", \"Cluster\"])\n",
    "        for u, v in hashGraphs.edges():\n",
    "            writer.writerow([u, v, hashGraphs[u][v][\"weight\"], hashGraphs[u][v][\"type\"], hashGraphs[u][v][\"hashType\"], hashGraphs[u][v][\"cluster\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to CSV\n",
    "writeToCSV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "regBefore= loadPickle('Regular_Before_py37.pkl')[['created_at', 'id_str']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "regAfter= loadPickle('Regular_After_py37.pkl')[['created_at', 'id_str']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "resBefore= loadPickle('Restored_Before_py37.pkl')[['created_at', 'id_str']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "resAfter= loadPickle('Restored_After_py37.pkl')[['created_at', 'id_str']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "regBefore[regBefore['id_str'].isin(regAfter['id_str'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the graph objects\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Changing the post was created to date time attribute\n",
    "regBeforePosts['createdAtTime']= pd.to_datetime(regBeforePosts['CreatedAt'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "# Sorting the users df based on the creation dates\n",
    "Posts.sort_values(by='createdAtTime', ascending= False, inplace= True)\n",
    "\n",
    "# Set the period to the particular time range\n",
    "periodWise={\n",
    "    'dateWise': Posts['createdAtTime'].dt.to_period('D').value_counts(), \n",
    "    'weekWise': Posts['createdAtTime'].dt.to_period('W').value_counts(), \n",
    "    'monthWise': Posts['createdAtTime'].dt.to_period('M').value_counts(), \n",
    "    'yearWise': Posts['createdAtTime'].dt.to_period('Y').value_counts()\n",
    "}\n",
    "\n",
    "#Define the colors\n",
    "colors= {\n",
    "    'dateWise': '#ff0000',\n",
    "    'weekWise': '#00ff00',\n",
    "    'monthWise': '#0000ff',\n",
    "    'yearWise': '#000000'\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize a figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# iterate in period to add the day types\n",
    "for i in periodWise:\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=list(periodWise[i].index.to_timestamp()),\n",
    "                y=list((100*periodWise[i].values)/len(Posts.index)),\n",
    "                marker_color=colors[i],\n",
    "                name=f\"{i} Wise\"))\n",
    "\n",
    "fig.update_layout(\n",
    "     xaxis=dict(\n",
    "        autorange=True,\n",
    "        rangeslider=dict(\n",
    "            autorange=True,\n",
    "        ),\n",
    "        type=\"date\"\n",
    "    ),\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            active= 0,\n",
    "            buttons=list(\n",
    "               [dict(label= \"Select Dropdown\", method=\"update\",args=[\n",
    "                            {\"visible\": [False]*4},\n",
    "                            {\"title\": f'Select Dropdown'}\n",
    "                        ])] + \n",
    "               [ \n",
    "                   dict(label= f\"Content Generation Pattern ({list(periodWise.keys())[i]} Wise)\",\n",
    "                     method=\"update\",\n",
    "                     args=[\n",
    "                            {\"visible\": [True if j==i else False for j in range(4)]},\n",
    "                            \n",
    "                            {\n",
    "                                \"title\": f'Content Generation Pattern ({list(periodWise.keys())[i]} Wise)',\n",
    "                                'xaxis': {'title': 'Date'},\n",
    "                                'yaxis': {'title': '% of content pushed'},\n",
    "                            }\n",
    "                        ]\n",
    "                    ) for i in range(4)\n",
    "                ]\n",
    "            ),\n",
    "        direction=\"down\",\n",
    "        pad={\"t\": -40},\n",
    "        showactive=True,\n",
    "        x=0.1,\n",
    "        xanchor=\"left\",\n",
    "        yanchor=\"top\"\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "# Set title\n",
    "fig.update_layout(title_text=\"Content Generation (Select from dropdown)\")\n",
    "\n",
    "# Plot the figure\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
